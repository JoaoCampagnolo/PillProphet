{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Compile a list of \"stock ticker\" identifiers, from the website stockanalysis.com. The list is incomplete. Contains only the larger companies, which might have implications down the line. There are 1175 companies listed in the file, and the file name is healthcare_stocks.csv"
      ],
      "metadata": {
        "id": "lBY7MMGIlnbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szocM1Wvk6Ql"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the website\n",
        "url = 'https://stockanalysis.com/stocks/sector/healthcare/'\n",
        "\n",
        "def scrape_table(url):\n",
        "    # Fetch the content from the URL\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the table\n",
        "    table = soup.find('table', {'class': 'symbol-table'})\n",
        "\n",
        "    # Prepare lists to store the data\n",
        "    no_list = []\n",
        "    symbol_list = []\n",
        "    company_name_list = []\n",
        "\n",
        "    # Iterate through each row of the table\n",
        "    for row in table.find_all('tr')[1:]:  # skip the header row\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) >= 3:\n",
        "            no_list.append(cols[0].text.strip())\n",
        "            symbol_list.append(cols[1].text.strip())\n",
        "            company_name_list.append(cols[2].text.strip())\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'No.': no_list,\n",
        "        'Symbol': symbol_list,\n",
        "        'Company Name': company_name_list\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Scrape the table\n",
        "df = scrape_table(url)\n",
        "\n",
        "# Specify the path within Google Drive where the CSV will be saved\n",
        "csv_file = '/content/drive/MyDrive/healthcare_stocks.csv'\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f'Data saved to {csv_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A previously compiled table of clinical trial preemptive reports (merged_clinical_trials.csv), with a unique NCT-number for each trial, is loaded and filtered for industry sponsors, as opposed to academic trials such as NHS etc. Then the sponsor name (company name) from the trial report is saved as \"company_names.csv\"."
      ],
      "metadata": {
        "id": "tANCQ-Dymhpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "file_path = '/content/drive/MyDrive/ClinicalTrialsFull/merged_clinical_trials.csv'\n",
        "df = pd.read_csv(file_path, encoding='utf-8', engine='python', on_bad_lines='skip')\n",
        "\n",
        "# Standardize column names to lower case and strip whitespace\n",
        "df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "# Check if 'funder type' column exists after standardization\n",
        "if 'funder type' not in df.columns:\n",
        "    raise KeyError(\"The column 'funder type' was not found in the dataset. Please check the column names.\")\n",
        "\n",
        "# Filter the rows where 'funder type' is 'industry'\n",
        "df_industry = df[df['funder type'] == 'INDUSTRY']\n",
        "\n",
        "# Extract unique company names (sponsors)\n",
        "company_names = df_industry['sponsor'].unique()\n",
        "\n",
        "# Write company names to a text file, tab-separated\n",
        "output_file_path = '/content/drive/MyDrive/ClinicalTrialsFull/company_names.txt'\n",
        "with open(output_file_path, 'w') as f:\n",
        "    f.write('\\t'.join(company_names))\n",
        "\n",
        "print(\"Company names saved successfully to company_names.txt.\")"
      ],
      "metadata": {
        "id": "KKrX5SyHmh96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the trial report table, and the stock ticker list. Match the sponsor (the company) with the appropriate stock ticker. Some variation in the company names can happen, so it creates a match if the first 5 letters of the company name corresponds with the name of a stock ticker.\n",
        "\n",
        "From the trial report table it finds the date of results being posted, and checks the average stock price 30 days before, and 30 days after. If the ratio is above 1, the price has increased from the results being posted, and the specific trial is assigned a parameter \"success\", and vice versa. The results are saved as \"drug_trial_results.csv\".\n",
        "\n",
        "It also saves a list of the companies that couldn't be assigned a stock ticker, to perhaps be matched up manually later (when we hire a student assistant)."
      ],
      "metadata": {
        "id": "IiFEkDiLnhSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas yfinance\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Function to load the dataset using different delimiters\n",
        "def load_csv_with_fallbacks(file_path):\n",
        "    delimiters = [',', ';', '\\t']  # List of common delimiters to try\n",
        "    for delimiter in delimiters:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, delimiter=delimiter, on_bad_lines='skip', engine='python')\n",
        "            print(f\"File loaded successfully using delimiter: '{delimiter}'\")\n",
        "            return df\n",
        "        except pd.errors.ParserError as e:\n",
        "            print(f\"ParserError with delimiter '{delimiter}': {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error with delimiter '{delimiter}': {e}\")\n",
        "    raise ValueError(\"Failed to load the CSV file with common delimiters.\")\n",
        "\n",
        "# Load the dataset\n",
        "csv_file_path = '/content/drive/MyDrive/ClinicalTrialsFull/merged_clinical_trials.csv'\n",
        "df = load_csv_with_fallbacks(csv_file_path)\n",
        "\n",
        "# Load the ticker mapping CSV file\n",
        "ticker_file_path = '/content/drive/MyDrive/healthcare_stocks.csv'\n",
        "df_tickers = pd.read_csv(ticker_file_path)\n",
        "\n",
        "# Create a dictionary to map company names to stock tickers based on the first 5 letters\n",
        "ticker_dict = {name[:5].lower(): ticker for name, ticker in zip(df_tickers['Company Name'], df_tickers['Symbol'])}\n",
        "\n",
        "# Standardize column names to lower case and strip whitespace\n",
        "df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "# Check if 'results first posted' column exists\n",
        "if 'results first posted' not in df.columns:\n",
        "    raise KeyError(\"The column 'results first posted' was not found in the dataset. Please check the column names.\")\n",
        "\n",
        "# Filter rows where 'funder type' is 'industry'\n",
        "df_industry = df[df['funder type'] == 'INDUSTRY']\n",
        "\n",
        "# Display the first few rows to check if the dataset is loaded correctly\n",
        "print(df_industry.head())\n",
        "\n",
        "# Initialize a list to collect results and a set to track unmatched companies\n",
        "results = []\n",
        "unmatched_companies = set()\n",
        "\n",
        "# Iterate through each row in the filtered DataFrame\n",
        "for index, row in df_industry.iterrows():\n",
        "    try:\n",
        "        nct_number = row['nct number']\n",
        "        sponsor = row['sponsor']\n",
        "        results_date = row['results first posted']\n",
        "\n",
        "        # Check if results_date is missing or NaN\n",
        "        if pd.isna(results_date):\n",
        "            print(f\"Results date is missing for NCT Number {nct_number}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Convert results_date to datetime\n",
        "        results_date = datetime.strptime(results_date, '%Y-%m-%d')\n",
        "\n",
        "        # Calculate 30 days before and after the results date\n",
        "        start_date = results_date - timedelta(days=30)\n",
        "        end_date = results_date + timedelta(days=30)\n",
        "\n",
        "        # Get the stock ticker by matching the first 5 letters of the sponsor name\n",
        "        ticker_symbol = ticker_dict.get(sponsor[:5].lower(), None)\n",
        "\n",
        "        if not ticker_symbol:\n",
        "            print(f\"No stock ticker found for sponsor: {sponsor}. Adding to unmatched list...\")\n",
        "            unmatched_companies.add(sponsor)\n",
        "            continue\n",
        "\n",
        "        # Fetch stock price data using yfinance\n",
        "        stock_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
        "\n",
        "        # Handle cases where the download might fail or return empty\n",
        "        if stock_data.empty:\n",
        "            print(f\"No data found for ticker: {ticker_symbol} for NCT Number {nct_number}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Calculate the average stock price 30 days before and 30 days after the results date\n",
        "        avg_before = stock_data.loc[:results_date].iloc[-30:]['Close'].mean()\n",
        "        avg_after = stock_data.loc[results_date:].iloc[:30]['Close'].mean()\n",
        "\n",
        "        # Calculate the ratio\n",
        "        ratio = avg_before / avg_after\n",
        "\n",
        "        # Determine success or failure\n",
        "        if ratio < 1:\n",
        "            status = 'Success'\n",
        "        else:\n",
        "            status = 'Failure'\n",
        "\n",
        "        # Collect the result\n",
        "        results.append({'NCT Number': nct_number, 'Sponsor': sponsor, 'Status': status})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for NCT Number {nct_number}: {e}\")\n",
        "\n",
        "# Convert the results list to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_csv_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"Results saved successfully to {results_csv_path}\")\n",
        "\n",
        "# Save unmatched companies to a separate CSV file\n",
        "unmatched_csv_path = '/content/drive/MyDrive/ClinicalTrialsCSV/unmatched_companies.csv'\n",
        "\n",
        "# If the file already exists, load existing unmatched companies into the set\n",
        "if os.path.exists(unmatched_csv_path):\n",
        "    existing_unmatched_df = pd.read_csv(unmatched_csv_path)\n",
        "    existing_unmatched_set = set(existing_unmatched_df['Unmatched Sponsor'])\n",
        "    unmatched_companies.update(existing_unmatched_set)\n",
        "\n",
        "# Convert the set to a DataFrame and save it\n",
        "unmatched_df = pd.DataFrame(list(unmatched_companies), columns=['Unmatched Sponsor'])\n",
        "unmatched_df.to_csv(unmatched_csv_path, index=False)\n",
        "print(f\"Unmatched sponsors saved successfully to {unmatched_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "fDidcMiZngZz",
        "outputId": "289e1f6d-5d84-4eaa-a8eb-183d4ff26ad7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.43)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b20ca8e6bed>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pandas yfinance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m from pandas.core.groupby import (\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.groupby.generic import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m from pandas.core.groupby import (\n\u001b[1;32m     69\u001b[0m     \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0msanitize_masked_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0;31m from pandas.core.generic import (\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mNDFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mmake_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m from pandas.core import (\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0malgorithms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0marraylike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mlength_of_indexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0;31m from pandas.core.indexes.api import (\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mMultiIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntervalIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the successes and failures, to verify that the ratio is somewhat decent. At the moment it is fairly equal, which is good, as we won't introduce bias in the training set and accidentally cause more type 1 or type 2 errors. On the other hand, it doesn't reflect the real world success/failure ratio, as only about 10% of trials succeed (across all phases. Our dataset is only phase 3 trials)"
      ],
      "metadata": {
        "id": "i0aZ5fpoqDbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the results CSV file\n",
        "results_csv_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "results_df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Count the number of successes and failures\n",
        "status_counts = results_df['Status'].value_counts()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "status_counts.plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('Number of Successes and Failures in Clinical Trials')\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Number of Trials')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m_Kz5bpjojpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot of the frequency of a company in our trial set. Eli Lilly, Merck, and Pfizer are sponsors of a huge proportion of the trials. This could have some implications down the line."
      ],
      "metadata": {
        "id": "nI-ic6kbrUL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the results CSV file\n",
        "results_csv_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "results_df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Count the frequency of each company (sponsor)\n",
        "company_frequency = results_df['Sponsor'].value_counts()\n",
        "\n",
        "# Select the top 20 companies by frequency\n",
        "top_companies = company_frequency.head(20)\n",
        "\n",
        "# Plot the company frequencies using a horizontal bar plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_companies.plot(kind='barh', color='skyblue')\n",
        "plt.title('Top 20 Companies by Frequency in Clinical Trials Dataset')\n",
        "plt.xlabel('Number of Trials')\n",
        "plt.ylabel('Company (Sponsor)')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the company with the most trials at the top\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_yAvBM0VrT5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency Inverse Document Frequency (TF-IDF) is utilized to establish features based on word frequency, and a \"Random Forest classifier\" model is trained to predict the success/failure based on the features. Accuracy, as reported by the output, is 55%. GPT interprets that the \"confusion matrix\" from the output tells us that it predicts successes better than failures (perfect, let's keep it that way). Perhaps more importantly, the output shows us the terms that are most correlated with success/failure. The top words include \"model,\" \"allocation,\" \"treatment\", which are terms describing study design, and \"plasma,\" \"cmax,\" \"dose,\" \"pharmacokinetics\", which describe specific outcome measures used in trials. Maybe we can decode which types of trials have higher success rates. \"plasma,\" \"cmax,\" \"dose,\" \"pharmacokinetics\" are outcome measures used in dose-determination studies, and toxicity/safety studies. They generally come before the efficacy studies, but could have stronger implications for the viability of the drug (if it's toxic, it's no good)."
      ],
      "metadata": {
        "id": "1L34Aab7s8Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas scikit-learn nltk\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the clinical trials and results datasets\n",
        "clinical_trials_path = '/content/drive/MyDrive/ClinicalTrialsFull/merged_clinical_trials.csv'\n",
        "trial_results_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "\n",
        "df_clinical_trials = pd.read_csv(clinical_trials_path)\n",
        "df_trial_results = pd.read_csv(trial_results_path)\n",
        "\n",
        "# Standardize column names to lower case and strip whitespace\n",
        "df_clinical_trials.columns = df_clinical_trials.columns.str.lower().str.strip()\n",
        "df_trial_results.columns = df_trial_results.columns.str.lower().str.strip()\n",
        "\n",
        "# Merge the two datasets on 'nct number'\n",
        "df_merged = pd.merge(df_clinical_trials, df_trial_results, on='nct number', how='inner')\n",
        "\n",
        "# Combine all relevant text columns into one\n",
        "text_columns = ['study title', 'brief summary', 'conditions', 'interventions',\n",
        "                'primary outcome measures', 'secondary outcome measures',\n",
        "                'other outcome measures', 'study design']\n",
        "df_merged['combined_text'] = df_merged[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Preprocess the combined text data\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "df_merged['processed_text'] = df_merged['combined_text'].apply(preprocess_text)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = tfidf_vectorizer.fit_transform(df_merged['processed_text']).toarray()\n",
        "y = df_merged['status'].apply(lambda x: 1 if x == 'Success' else 0)  # Encode success as 1, failure as 0\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Identify important features\n",
        "feature_importances = model.feature_importances_\n",
        "important_features = sorted(zip(tfidf_vectorizer.get_feature_names_out(), feature_importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print top 20 important features\n",
        "print(\"Top 20 important features correlated to success/failure:\")\n",
        "for feature, importance in important_features[:20]:\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2r1WZPQCs5Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out some more strategies:\n",
        "Use BERT to create contextual embeddings of the trial descriptions. BERT captures the meaning and relationships between words\n",
        "Use sentiment analysis to extract \"the vibe\".\n",
        "\n",
        "Several models are run, being; Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machine (SVM), and Neural Network (MLP) models.\n",
        "An ensemble method (Random Forest) is also trained and evaluated.\n",
        "\n",
        "Results show that none of the models significantly outperform the others. Seems like the optimization needs to happen at the feature extraction level, or at the dataset level (Maybe we need to find some more information-dense training data?). The models perform at around 55-57% accuracy, which is not great, but still, anything above random will average to a profit given enough time (if we deploy it to trading live)."
      ],
      "metadata": {
        "id": "vjYuXzqIwoNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas scikit-learn nltk transformers tensorflow keras\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load datasets\n",
        "clinical_trials_path = '/content/drive/MyDrive/ClinicalTrialsFull/merged_clinical_trials.csv'\n",
        "trial_results_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "\n",
        "df_clinical_trials = pd.read_csv(clinical_trials_path)\n",
        "df_trial_results = pd.read_csv(trial_results_path)\n",
        "\n",
        "# Standardize column names\n",
        "df_clinical_trials.columns = df_clinical_trials.columns.str.lower().str.strip()\n",
        "df_trial_results.columns = df_trial_results.columns.str.lower().str.strip()\n",
        "\n",
        "# Merge datasets on 'nct number'\n",
        "df_merged = pd.merge(df_clinical_trials, df_trial_results, on='nct number', how='inner')\n",
        "\n",
        "# Combine text columns for analysis\n",
        "text_columns = ['study title', 'brief summary', 'conditions', 'interventions',\n",
        "                'primary outcome measures', 'secondary outcome measures',\n",
        "                'other outcome measures', 'study design']\n",
        "df_merged['combined_text'] = df_merged[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "df_merged['processed_text'] = df_merged['combined_text'].apply(preprocess_text)\n",
        "\n",
        "# Add sentiment analysis\n",
        "df_merged['sentiment_score'] = df_merged['processed_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "# Advanced feature extraction using BERT embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=512)\n",
        "    outputs = bert_model(inputs)\n",
        "    return np.mean(outputs.last_hidden_state[0].numpy(), axis=0)\n",
        "\n",
        "df_merged['bert_embedding'] = df_merged['processed_text'].apply(get_bert_embedding)\n",
        "\n",
        "# Prepare features and target variable\n",
        "X_text = np.array(df_merged['bert_embedding'].tolist())\n",
        "X_sentiment = df_merged[['sentiment_score']].values\n",
        "X = np.hstack((X_text, X_sentiment))\n",
        "y = df_merged['status'].apply(lambda x: 1 if x == 'Success' else 0)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different machine learning models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear'),\n",
        "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Results for {model_name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Ensemble method (stacking)\n",
        "ensemble_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "print(\"Results for Ensemble Model (Random Forest):\")\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "print(confusion_matrix(y_test, y_pred_ensemble))\n"
      ],
      "metadata": {
        "id": "w0kL2ZARwn8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the feature extraction level, i decided to try to use medicine specific BERT models (ClinicalBERT, BioBERT), and something called \"aspect-based sentiment\", to see if that can more accurately identify meaningful features in the trials, as it might understand the language better.\n",
        "\n",
        "No significant performace increase (55-58% now)."
      ],
      "metadata": {
        "id": "zkKaIKThzZZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas scikit-learn transformers tensorflow keras torch nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load datasets\n",
        "clinical_trials_path = '/content/drive/MyDrive/ClinicalTrialsFull/merged_clinical_trials.csv'\n",
        "trial_results_path = '/content/drive/MyDrive/ClinicalTrialsCSV/drug_trial_results.csv'\n",
        "\n",
        "df_clinical_trials = pd.read_csv(clinical_trials_path)\n",
        "df_trial_results = pd.read_csv(trial_results_path)\n",
        "\n",
        "# Standardize column names\n",
        "df_clinical_trials.columns = df_clinical_trials.columns.str.lower().str.strip()\n",
        "df_trial_results.columns = df_trial_results.columns.str.lower().str.strip()\n",
        "\n",
        "# Merge datasets on 'nct number'\n",
        "df_merged = pd.merge(df_clinical_trials, df_trial_results, on='nct number', how='inner')\n",
        "\n",
        "# Combine text columns for analysis\n",
        "text_columns = ['study title', 'brief summary', 'conditions', 'interventions',\n",
        "                'primary outcome measures', 'secondary outcome measures',\n",
        "                'other outcome measures', 'study design']\n",
        "df_merged['combined_text'] = df_merged[text_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "df_merged['processed_text'] = df_merged['combined_text'].apply(preprocess_text)\n",
        "\n",
        "# Load BioBERT and ClinicalBERT Tokenizers and Models\n",
        "biobert_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "biobert_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "clinicalbert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "clinicalbert_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "# Function to get BioBERT and ClinicalBERT embeddings\n",
        "def get_bert_embedding(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return np.mean(outputs.last_hidden_state[0].numpy(), axis=0)\n",
        "\n",
        "df_merged['biobert_embedding'] = df_merged['processed_text'].apply(lambda x: get_bert_embedding(x, biobert_tokenizer, biobert_model))\n",
        "df_merged['clinicalbert_embedding'] = df_merged['processed_text'].apply(lambda x: get_bert_embedding(x, clinicalbert_tokenizer, clinicalbert_model))\n",
        "\n",
        "# Aspect-Based Sentiment Analysis: Define aspects\n",
        "aspects = ['efficacy', 'safety', 'side effects', 'quality of life']\n",
        "\n",
        "def get_aspect_sentiment(text, aspect):\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\n",
        "    truncated_text = (text[:512] + '..') if len(text) > 512 else text\n",
        "    result = sentiment_pipeline(f\"{aspect}: {truncated_text}\", max_length=512, truncation=True)[0]\n",
        "    return result['score']\n",
        "\n",
        "# Apply aspect-based sentiment analysis\n",
        "for aspect in aspects:\n",
        "    df_merged[f'sentiment_{aspect}'] = df_merged['processed_text'].apply(lambda x: get_aspect_sentiment(x, aspect))\n",
        "\n",
        "# Prepare features and target variable\n",
        "X_biobert = np.array(df_merged['biobert_embedding'].tolist())\n",
        "X_clinicalbert = np.array(df_merged['clinicalbert_embedding'].tolist())\n",
        "X_sentiment_aspects = df_merged[[f'sentiment_{aspect}' for aspect in aspects]].values\n",
        "X = np.hstack((X_biobert, X_clinicalbert, X_sentiment_aspects))\n",
        "\n",
        "y = df_merged['status'].apply(lambda x: 1 if x == 'Success' else 0)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different machine learning models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear'),\n",
        "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Results for {model_name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Ensemble method (stacking)\n",
        "ensemble_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "print(\"Results for Ensemble Model (Random Forest):\")\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "print(confusion_matrix(y_test, y_pred_ensemble))\n"
      ],
      "metadata": {
        "id": "cuuhAmb-zX25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps?\n",
        "-LSTM (Long Short-Term Memory) models?\n",
        "-BERT based transformer?\n",
        "-Deep learning model?\n",
        "-Advanced topic extraction with Latent Dirichlet Allocation (LDA)? This can extract themes and meanings \"between the lines\" so to say.\n",
        "-Deploy pre-trained LLM's to extract parameters/features through API access. https://huggingface.co/ is open source and provides many LLM's for free within limits.\n",
        "-Consider pulling data from the internet/pubmed/press-releases/twitter that relates to the trial drug, to gather super nuanced data sets, to incorporate into the analysis. Perhaps LLM's can automate that. (feature extraction from CSV -> Search web for related stuff -> feature extraction from web material -> Success/failure).  "
      ],
      "metadata": {
        "id": "QsodLPU_0mdv"
      }
    }
  ]
}